\section{Related Work}

The landscape of neural network verification is broadly divided into complete methods, which provide exact certification at the cost of scalability, and incomplete methods, which utilize abstract interpretation to achieve efficiency.


Initial efforts in the field prioritized exactness. \citet{katz2017reluplex} developed \texttt{Reluplex}, an SMT-based solver that extends the Simplex algorithm to manage the piecewise linear nature of ReLU networks. In a similar vein, \citet{tjeng2019evaluating} modeled the verification problem using Mixed Integer Linear Programming (\texttt{MIPVerify}) to determine precise adversarial boundaries. Although these approaches guarantee soundness and completeness, their NP-complete complexity renders them intractable for modern, large-scale architectures.

To overcome the scalability bottlenecks of complete verifiers, Abstract Interpretation \cite{cousot_radhia_cousot_1977} has been widely adopted. \citet{Gehr2018AI2SA} introduced \texttt{AI2}, a framework employing zonotopes for bound propagation. This was advanced by \citet{singh_gehr_p√ºschel_vechev_2019} with \texttt{DeepPoly}, which combines intervals with polyhedral constraints to handle non-linearities like Sigmoid and Tanh through linear relaxation. While \texttt{DeepPoly} significantly improves scalability, it suffers from the "wrapping effect," where approximation errors accumulate with network depth. \citet{Weng2018TowardsFC} similarly utilized linear bound propagation to expedite robustness certification.


While the theory of dual numbers is well-established for derivative evaluation \cite{griewank2008evaluating}, its utility in certifying robustness has been limited. Unlike purely geometric abstractions, our method uses Abstract Duals to compute global Lipschitz bounds. This allows us to bypass layer-wise error accumulation in specific regimes, offering a distinct advantage in shallow networks where gradient information remains coherent.
