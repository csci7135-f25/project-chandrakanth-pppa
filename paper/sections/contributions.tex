\section{Contributions}

\subsection{Algorithmic Realization of Abstract Duals}
While the theoretical foundation rests on dual numbers, our practical contribution is the algorithmic realization of these concepts within the \textbf{Affine Arithmetic} domain. We implemented a custom OCaml module \texttt{AbstractDual} that handles the propagation of affine forms.
Crucially, for non-linear activations $\sigma$, we implement a sound linearization that explicitly accounts for the curvature error. As seen in our implementation, the radius of the value component is expanded by a quadratic term derived from the Taylor expansion:
\begin{equation}
    r_{new} = |\sigma'(c)| \cdot r_{old} + \underbrace{0.5 \cdot \max_{z} |\sigma''(z)| \cdot r_{old}^2}_{\text{Linearization Error}}
\end{equation}
This ensures that the abstract transformer remains sound even when the function has significant curvature, a detail often omitted in standard dual number formulations but critical for verification.

\subsection{Gradient-Based Verification Logic}
We formulate a robust verification condition based on the computed gradient bounds. Let $f_c(x)$ be the score of the correct class and $f_j(x)$ be the score of a competing class. The robustness margin is $M(x_0) = f_c(x_0) - f_j(x_0)$.
To certify robustness over $\mathbb{B}_\infty(x_0, \epsilon)$, we compute the global Lipschitz constant $K$ of the margin function $m(x) = f_c(x) - f_j(x)$ using our abstract duals. The verification condition implemented in our tool is:
\begin{equation}
    M(x_0) > \epsilon \cdot \sup_{x \in \mathbb{B}} ||\nabla m(x)||_1
\end{equation}
In our implementation, we conservatively estimate this by summing the worst-case L1 norms of the gradients for the correct and competing classes:
\begin{equation}
    \text{Certified Variation} = \epsilon \cdot \sum_{i} \max(|g_{low}^{(i)}|, |g_{high}^{(i)}|)
\end{equation}
If the initial margin exceeds twice this variation (accounting for the worst-case drop in $f_c$ and rise in $f_j$), the network is provably robust.

\begin{algorithm}
\caption{Robustness Verification via Abstract Duals}
\label{alg:verification}
\begin{algorithmic}[1]
\Require Neural Network $f$, Input $x_0$, Label $y$, Radius $\epsilon$
\Ensure \textbf{True} if robust, \textbf{False} otherwise

\State \textbf{Step 1: Abstract Initialization}
\State $\hat{x}_{val} \gets \text{AffineForm}(x_0 - \epsilon, x_0 + \epsilon)$ \Comment{Input Box}
\State $\hat{x}_{grad} \gets \text{Identity}(n_{in})$ \Comment{Gradient w.r.t inputs}
\State $\mathcal{X} \gets \langle \hat{x}_{val}, \hat{x}_{grad} \rangle$

\State \textbf{Step 2: Forward Propagation}
\For{layer $l$ in $f$}
    \If{$l$ is Linear($W, b$)}
        \State $\hat{x}_{val} \gets W \hat{x}_{val} + b$
        \State $\hat{x}_{grad} \gets W \hat{x}_{grad}$
    \ElsIf{$l$ is Activation($\sigma$)}
        \State $c \gets \text{center}(\hat{x}_{val})$
        \State $r \gets \text{radius}(\hat{x}_{val})$
        \State $\epsilon_{err} \gets 0.5 \cdot \max |\sigma''| \cdot r^2$ \Comment{Curvature Error}
        \State $\hat{x}_{val} \gets \sigma(c) + \sigma'(c)(\hat{x}_{val} - c) + \epsilon_{err}$
        \State $\hat{\sigma}' \gets \text{AffineApprox}(\sigma', \hat{x}_{val})$
        \State $\hat{x}_{grad} \gets \hat{\sigma}' \otimes \hat{x}_{grad}$ \Comment{Affine Multiplication}
    \EndIf
\EndFor

\State \textbf{Step 3: Certification}
\State $K \gets \sup ||\hat{x}_{grad}||_1$ \Comment{Global Lipschitz Bound}
\State $M \gets f(x_0)_y - \max_{j \neq y} f(x_0)_j$ \Comment{Concrete Margin}
\If{$M > \epsilon \cdot K$}
    \State \Return \textbf{True}
\Else
    \State \Return \textbf{False}
\EndIf
\end{algorithmic}
\end{algorithm}

\subsection{Implementation and Stability Analysis}
We give a functional implementation of our verification environment within OCaml, making use of a Monadic approach to cope with the computational graph. The distinct part of our tool is the \textit{Gradient Stability Check}, used to identify the ``wrapping effect'' in real-time. The metric analyzes the intervals of the gradients on all dimensions corresponding to the inputs. If the interval $[\underline{g}, \overline{g}]$ strictly contains zero, it marks a neuron as unstable. This metric, $\mathcal{I}_{unstable}$, highly correlates with failure cases of relational domains such as $\texttt{DeepPoly}$, providing a novel way of choosing verification heuristics.